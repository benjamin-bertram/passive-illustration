[
    {
        "title": "Overview",
        "text": "<p>This overview introduces the fundamentals of image generation through <strong>Stable Diffusion</strong>, a process that relies on a series of interconnected algorithms, each contributing to the final output. While many AI image-generation platforms emphasize ease of use, they often limit the full potential of AI-driven artwork. These platforms focus on familiar aesthetics, confining creativity within predefined options, making their outputs akin to readymade pieces designed for quick consumption. This approach barely scratches the surface of what generative AI can achieve.</p><p>Generative AI images and aesthetics differ <i>fundamentally</i> from previous art forms, similar how the camera differed fundamentally from traditional painting. Understanding its mechanics is essential for unlocking deeper creative possibilities. By learning how each layer of the diffusion process works, artists can experiment beyond the preset buttons, pushing the boundaries of what's possible. Much like mastering a camera is vital for photography, understanding how AI regenerates images from vast datasets is crucial for effective creative control. After all, in this new era of digital creation, ‚Äúdata is the new pigment‚Äù (<i>Refik Anadol</i>).</p><p>Although many of these layers have subsettings of their own, we will focus on the main parts that can be altered in user interfaces such as <i>ComfyUI</i> or <i>Automatic1111</i>.</p>",
        "image": "public/timeline/sdtools.webp"
    },
    {
        "title": "Latent Space",
        "text": "<p><strong>Latent Space</strong> is a compressed, lower-dimensional representation of data used in generative models like <i>Latent Diffusion</i>. In image generation, latent space holds abstract information about the image while significantly reducing its complexity. Rather than operating on millions of pixels, models like <i>Stable Diffusion</i> encode the image into latent space, capturing essential features and discarding redundant information. This allows the model to generate high-quality outputs while being computationally efficient.</p><p>Navigating latent space offers flexibility for tasks such as image interpolation. It enables the model to blend or morph between concepts by moving through this abstract space without needing to store all possible combinations. This approach allows generative models to explore creative variations while maintaining coherence and structure.</p><p>To visualize this, imagine a simple two-dimensional latent space where one dimension represents color (üü® yellow, üü• red, üü¶ blue, üü© green) and another represents shape (‚ñµ triangle, ‚ñ° square, ‚óã circle, and a line). Moving through this space lets the model combine different colors and shapes without needing every combination explicitly stored. If we add another dimension, like shininess, we move into a three-dimensional space. However, in real-world applications like Stable Diffusion, the latent space contains 768 dimensions, making it impossible to visualize directly. This high-dimensional space allows the model to capture subtle variations and complexities in the data, leading to more nuanced and detailed image generation.</p>",
        "image": ""
    },
    {
        "title": "GANs",
        "text": "<p><strong>Generative Adversarial Networks</strong> (GANs) consist of two neural networks‚Äîa generator and a discriminator‚Äîthat work in competition with one another.</p><ul><li><strong>Generator</strong>: This network creates synthetic data, such as images, starting from random noise. Its goal is to generate data that mimics real-world data.</li><li><strong>Discriminator</strong>: This network evaluates the generated data against actual training data, determining whether the output is real or fake. It assigns a value between 0 (perfectly aligned with real data) and 1 (completely different).</li></ul><p>During training, the generator learns to produce images that progressively lower the discriminator's score. Once trained, the generator is capable of producing images that closely resemble the original dataset.</p><p>One major advantage of GANs over diffusion models is their speed and efficiency. But while attempts have been made to combine GANs with models like <i>CLIP</i> for text-to-image generation, the lack of diversity in GAN outputs and the complex training process have limited their effectiveness in producing high-fidelity text-based images.</p><p>However, GANs remain valuable in certain use cases, such as image upscaling (resizing images with minimal quality loss) and artistic applications. Artists like <i>Refik Anadol</i> and <i>√úm√ºt Yildiz</i> use GANs (e.g., <i>StyleGAN</i>) for creative effects such as ‚Äú<i>latent walks</i>‚Äù, where the generated shapes smoothly morph between different forms, creating unique visual experiences.</p>",
        "image": "public/how_genai_works/gan.webp"
    },
    {
        "title": "Diffusion Networks",
        "text": "<p><strong>Diffusion models</strong> are a class of deep learning models that generate high-quality images by gradually denoising random Gaussian noise. Developed by researchers at the University of California, Berkeley, these models have gained significant attention in the field of generative AI. Diffusion models work by learning to reverse a gradual noising process, effectively reconstructing complex data distributions from noise.</p><p>The process involves two main steps: <i>forward diffusion</i> and <strong>reverse diffusion</strong>. In forward diffusion, small amounts of Gaussian noise are iteratively added to an image until it becomes pure noise. The reverse diffusion process, which is what the model learns, attempts to undo this noising process step by step, eventually producing a clear image from the noise.</p><p>Like GANs, diffusion models are <i>non-conditional</i> models, meaning they generate random images. <i>OpenAI's</i> <a target='_blank' rel='noopener noreferrer' href='https://github.com/openai/guided-diffusion'><strong>Guided Diffusion</strong></a> allows these models to be guided by a text prompt. When generating images, guided diffusion models typically use a technique called <i>classifier-free guidance</i>. This method allows the model to balance between adhering to the learned image distribution and adhering to specific text prompts or constraints. By adjusting the guidance scale, users can control how closely the generated image follows the given prompt, thus balancing between creativity and adherence to the prompt. A guidance scale of zero basically means that the diffusion model is unguided again and generates a random image.</p>",
        "image": "public/how_genai_works/diffusion.webp"
    },
    {
        "title": "CLIP",
        "text": "<p>CLIP (<strong>Contrastive Language-Image Pretraining</strong>) is a neural network developed by <i>OpenAI </i>that links images and text within a shared latent space, enabling it to understand and match textual descriptions with visual content. Trained on large datasets of image-text pairs, CLIP interprets both images and text by converting them into numerical representations (<i>embeddings</i>). This allows the model to perform tasks like image retrieval or generating visuals based on textual prompts, by learning the associations between words and images.</p><p>The <strong>tokenization</strong> process is key to how CLIP interprets text. Words or phrases are transformed into numeric tokens, which the model uses for understanding text inputs. However, tokenization can break down compound words, which can alter the interpretation. For example, ‚Äúdreamhouse‚Äù might be split into two tokens, ‚Äúdream‚Äù and ‚Äúhouse,‚Äù while a phrase like ‚Äúdream beach‚Äù tokenizes differently due to the space between the words. This subtle difference in tokenization influences how CLIP associates images with the input text.</p><p>In models like Stable Diffusion, CLIP operates with a vocabulary of about 50,000 tokens, but because of unsupervised training, some tokens do not have direct real-world equivalents (such as ‚Äúsmore‚Äù, ‚Äúple‚Äù or <i>Instagram</i> hashtags like ‚Äúthinkbigsundaywithmarsha‚Äù) leading to unpredictable outputs. Understanding these quirks in tokenization and vocabulary is crucial for getting the best results when using CLIP in text-to-image generation.</p>",
        "image": "public/how_genai_works/clip.webp"
    },
    {
        "title": "UNET",
        "text": "<p>In the Stable Diffusion process, the <strong>U-Net</strong> architecture plays a crucial role in denoising and image generation. U-Net is a type of <strong>convolutional neural network</strong> (CNN) that was originally designed for biomedical image segmentation by <i>Olaf Ronneberger</i> at the University of Freiburg, but it has been adapted for generative tasks in models like Stable Diffusion.</p><p>The U-Net in Stable Diffusion is responsible for <i>guiding</i> the denoising process, where the model gradually removes noise from an image starting from random noise. This happens over a series of <i>diffusion steps</i>. The U-Net architecture excels at this task because of its <strong>encoder-decoder structure</strong>, which allows it to capture both local and global features. The encoder compresses the input image to capture high-level abstract features, while the decoder progressively refines the image, upsampling details and reducing noise.</p><p>A critical enhancement to this process is <strong>cross-attention</strong>, which allows the U-Net to incorporate the user's text prompt into the image generation. Cross-attention layers connect the text embeddings (via CLIP) to the image's latent space, guiding the model to focus on different parts of the text at various stages of denoising. This ensures that the generated image accurately reflects the semantic meaning of the input text, allowing for creative and contextually rich outputs.</p><p>The U-Net's design and functionality are key to the stability and performance of the diffusion process, enabling precise control over how images are generated and refined.</p>",
        "image": "public/how_genai_works/unet.webp"
    },
    {
        "title": "VAE and LDMs",
        "text": "<p><strong>Latent Diffusion Models </strong>(LDMs) represent a significant advancement over earlier generative models like <i>GANs</i> and standard diffusion models. Initially, diffusion models worked directly on pixel data, which made them computationally intensive and costly to use. LDMs, developed by <i>Rombach</i> et al., introduced a more efficient approach by operating in a compressed latent space via a variational <strong>autoencoder</strong> (VAE). Once the generation in latent space is complete, a decoder reconstructs the final high-resolution image from this compressed representation, making the process faster and more scalable.</p><p>The reason this compression works so effectively is rooted in the regularity of natural images. Unlike random data, images of faces, animals, or landscapes follow specific spatial patterns and relationships, allowing them to be compressed without significant loss of information (<strong>manifold hypothesis</strong>). Thus, LDMs can reduce the dimensionality of images while still preserving key details needed to generate realistic and detailed outputs.</p><p>In models like <strong>Stable Diffusion</strong>, this latent diffusion approach is key to producing high-quality images with limited computational resources. By operating in a latent space, Stable Diffusion can generate detailed images more efficiently, even on consumer hardware. This balance between image quality, control, and resource efficiency has made LDMs popular for text-to-image generation.</p>",
        "image": "public/how_genai_works/vae.webp"
    },
    {
        "title": "Summary",
        "text": "<p>Now let's summarize the generation process of an image:</p><ol><li><strong>Text Input to Latent Space:</strong> The process begins when the user inputs a text prompt. The <strong>CLIP Tokenizer</strong> translate this prompt into a series of tokens, converting the text into a numerical representation (an <strong>embedding</strong>) that the AI can understand. This text embedding is crucial for guiding the generative process, as it defines what the final image should resemble. Once the text is embedded, the model maps it to the <strong>latent space</strong>, a compressed representation that captures the essential features without needing to store every possible combination of pixel data. Latent space is where the core image concepts are formed.</li><li><strong>Diffusion Process with U-Net:</strong> In the <strong>latent space</strong>, the Stable Diffusion model uses a process of <strong>denoising</strong>. The <strong>U-Net architecture</strong> plays a key role here by taking random noise and iteratively refining it into a coherent image. Over several <strong>diffusion steps</strong>, the U-Net removes noise from the latent representation, guided by the CLIP embedding and gradually turning abstract noise into a detailed image which resembles the prompt.</li><li><strong>Decoding and Final Output:</strong> Once the image in the latent space has been sufficiently refined, the <strong>VAE Decoder</strong> takes over, transforming this compressed latent representation back into a high-resolution image. This is where the final image emerges, showcasing the rich details and nuances that were encoded in the latent space.</li></ol>",
        "image": "public/how_genai_works/stablediffusion.webp"
    }
]