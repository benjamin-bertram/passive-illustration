[
    {
        "title": "Steps",
        "text": "<p><strong>Stable Diffusion</strong> is basically a text-to-image model that generates images based on a text prompt. It belongs to a class of AI models known as <i>diffusion models</i>, which are trained on large datasets where each image is progressively diffused into noise through a process called forward diffusion. The model learns to reverse this process, known as reverse diffusion, to reconstruct the original image. By mastering this reversal, the model can generate new, coherent images from random noise that resemble the training data.</p><p>To create an image, Stable Diffusion starts with a completely random pattern in a latent space. The model then uses the reverse diffusion process, step by step, to remove noise from this initial pattern, gradually revealing an image that matches the given text prompt. Since Stable Diffusion is a conditional image generator, it uses the prompt to guide this denoising process. This allows the model to generate different images from the same starting noise, based on the input prompt.</p><p>The sampling process in Stable Diffusion involves an algorithm that controls how noise is removed at each step. Initially, larger steps are taken to set the global structure or composition of the image. As the process continues, the steps become smaller to refine finer details, such as textures and edges. This gradual refinement means that in early steps the image starts to emerge from the noise but remains blurry. With each step, the image becomes clearer, and around 20 steps, the model converges, focusing on enhancing fine details rather than altering the overall composition.</p>",
        "video": "public/settings/steps.mp4",
        "initialSliderIndex": 20,
        "slidervalue": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
    },
    {
        "title": "Seed",
        "text": "<p>The random noise pattern, which gets denoised step by step in Stable Diffusion, is first initialized by a number called a <strong>seed</strong>. This seed is not entirely random; it can be manually set or left to be randomly generated by the system. If a seed is not specified, Stable Diffusion generates a random one.</p><p>Using a random seed is beneficial when you want to explore a wide range of images generated from the same prompt, as it introduces variability. However, setting a specific seed allows you to generate reproducible images. This feature is particularly useful when you want to experiment with other parameters, make slight variations to a prompt, or share your work with others who can reproduce the exact same image.</p><p>The key advantage of using a fixed seed is that image generations with the same prompt, parameters, and seed will produce<i> exactly the same images every time</i>. This consistency enables you to create multiple similar variations of an image, conduct controlled experiments, or ensure that specific outputs can be reliably reproduced.</p><p>In the Stable Diffusion community, <i>some seeds</i> have been identified as having a higher probability of producing images with specific characteristics, such as particular color palettes or compositions. Knowing these seeds allows users to achieve more targeted outcomes, which can be helpful when looking for specific artistic results or replicating a particular style.</p>",
        "video": "public/settings/seed.mp4",
        "initialSliderIndex": 4,
        "slidervalue": ["Seed: 1","Seed: 2","Seed: 3","Seed: 42","Seed: 100","Seed: 1000","Seed: 1337","Seed: 21021986","Seed: 849238882843","Seed: 5492988827"]
    },
    {
        "title": "CFG-Scale",
        "text": "<p>Simply put, the CFG scale (<strong>Classifier Free Guidance scale</strong>), or guidance scale, is a parameter that controls how closely the image generation process follows the text prompt. Unlike <i>traditional diffusion</i> models, which use a separate classifier to guide the model toward specific outputs (like generating an image of a cat when prompted), Classifier-Free Guidance integrates this guidance directly within the diffusion model. The model is trained in both 'guided' (with a prompt) and 'unguided' (with no prompt) modes, allowing it to learn how to generate images that either closely follow or loosely interpret the text prompt.</p><p>The CFG scale essentially balances the <i>fidelity</i> of the output with <i>creative freedom</i>, allowing users to fine-tune how literally the AI interprets the input text. Higher CFG values make the model adhere more strictly to the prompt, producing images that closely match the specified details. However, at very high CFG values, the images can become overly literal, leading to potential issues like oversaturation, loss of detail, or artifacts, especially if the prompt is too restrictive or lacks sufficient descriptive detail.</p><p>Finding the optimal CFG value is often a matter of experimentation and balancing between prompt fidelity and image quality. Typically, a CFG value between 7 and 11 is considered ideal for most use cases, as it strikes a balance between following the prompt closely and maintaining high visual quality. If the CFG value is set too high, the generated image might suffer from excessive sharpness or noise; if set too low, the output may drift too far from the intended prompt.</p>",
        "video": "public/settings/cfg.mp4",
        "initialSliderIndex": 14,
        "slidervalue": [1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,6.5,7,7.5,8,8.5,9,9.5,10,10.5,11,11.5,12,12.5,13,13.5,14,14.5,15,15.5,16,16.5,17,17.5,18,18.5,19,19.5,20]
    },
    {
        "title": "Weight",
        "text": "<p>In the context of Stable Diffusion and other machine learning models, <i>embeddings</i> are numerical representations of concepts — such as words, phrases, or images — that capture their meanings in a high-dimensional vector space. You can modify how strongly the model emphasizes certain embeddings in the prompt using <strong>weights</strong>.</p><p>For example, in a prompt like <code>(sunset:1.5)(beach:0.75)</code>, the model prioritizes 'sunset' (weight 1.5) over 'beach' (weight 0.75), resulting in an image with more sunset elements but still some beach aspects.</p><p>There are different techniques for using weights:</p><ul><li><strong>Numerical Weights</strong>: You can directly specify numerical weights in your prompt to fine-tune how much attention is given to specific words. <code>(dog:2)</code> doubles the model's focus on 'dog', while <code>(cat:0.5)</code> reduces the focus on 'cat' by half.</li><li><strong>Blending Multiple Concepts</strong>: <code>((Donald Duck:0.5), (Knight:0.9), (E.T.:1.2))</code> blends the concepts together over the course of the image generation. This technique can be used to create images with properties of all concepts.</li><li><strong>Keyword Switching and Blending</strong>: <code>[Donald Duck:Mickey Mouse:0.5]</code> will transition the image from one featuring Donald Duck to one featuring Mickey Mouse halfway through the sampling steps. This method is effective for creating hybrid images.</li></ul><p>You can also use negative weights to decrease the influence of a word. If you increase or decrease the word too much, you get over saturated images or distorted features.</p>",
        "video": "public/settings/weight.mp4",
        "initialSliderIndex": 18,
        "slidervalue": ["man: -8.0", "man: -7.5", "man: -7.0", "man: -6.5", "man: -6.0", "man: -5.5", "man: -5.0", "man: -4.5", "man: -4.0", "man: -3.5", "man: -3.0", "man: -2.5", "man: -2.0", "man: -1.5", "man: -1.0", "man: -0.5", "man: 0.0", "man: 0.5", "man: 1.0", "man: 1.5", "man: 2.0", "man: 2.5", "man: 3.0", "man: 3.5", "man: 4.0", "man: 4.5", "man: 5.0", "man: 5.5", "man: 6.0", "man: 6.5", "man: 7.0", "man: 7.5", "man: 8.0"]
    },
    {
        "title": "Sampler",
        "text": "<p><strong>Samplers</strong> play a crucial role in the image generation process by determining <i>how</i> noise is removed from the initial random input to produce a clear and coherent image. During the last years people came up with different approaches to achieve this, impacting both the speed of generation and the quality of the final output.</p><p>Samplers in Stable Diffusion vary in their approach and performance:</p><ul><li><strong>Euler</strong> and <strong>Heun</strong> are classical methods for solving differential equations, with Euler being fast but less detailed and Heun offering more accuracy at a slower pace. </li><li><strong>LMS</strong> and <strong>PLMS</strong> improve accuracy by averaging steps but may introduce noise at lower steps. </li><li>The <i>DPM family</i>, including <strong>DPM</strong>, <strong>DPM2</strong>, and <strong>DPM++</strong>, reduces the number of steps needed for high-quality outputs, with DPM++ balancing speed and accuracy efficiently. </li><li><strong>DDIM</strong> provides quick, high-quality images but requires more steps for optimal results. </li><li><i>Ancestral samplers</i> like <strong>Euler-A </strong>add randomness, creating creative but less stable outputs.</li><li>The newer <strong>UniPC</strong> sampler efficiently combines prediction and correction to achieve high-quality results with fewer steps.</li></ul><p>Choosing the right sampler depends on the desired balance between speed and image quality. By experimenting with different samplers, you can optimize the trade-offs between computational efficiency and the quality of the generated images, tailoring the output to their specific needs and creative goals.</p>",
        "video": "public/settings/sampler.mp4",
        "initialSliderIndex": 18,
        "slidervalue": ["DPM++ 2M", "DPM++ SDE", "DPM++ 2M SDE", "DPM++ 2M SDE Heun", "DPM++ 2S a", "DPM++ 3M SDE", "Euler a", "Euler", "LMS", "Heun", "DPM2", "DPM2 a", "DPM fast", "DPM adaptive", "Restart", "DDIM", "DDIM CFG++", "PLMS", "UniPC", "LCM"]
    },
    {
        "title": "Denoising Strength",
        "text": "<p>During the generation process, the algorithm removes noise step by step (<i>denoising</i>). With the <strong>denoising strength</strong>, you control <i>how much</i> noise is removed during image generation. This parameter ranges from 0 to 1: a value of 0 retains most of the original input with minimal changes, while a value of 1 completely transforms the input, creating an entirely new image. Instead of starting with random noise, you can also provide an image as input. A denoising strength of 0 will keep the input image largely intact, while a value of 1 will generate a new image based on the noise.</p><p>When using an input image, a low denoising strength (close to 0) preserves most of the original image, making subtle edits. Higher values (closer to 1) introduce more variation, which is useful for significant alterations. This has several applications: in <strong>img2img</strong>, denoising strength controls how much the output deviates from the input image, with low values used for minor tweaks and high values for major changes. In <strong>inpainting</strong>, moderate values are recommended to ensure new content blends seamlessly with the existing image.</p><p>Adjusting the denoising strength balances detail preservation with creative transformation, allowing for both subtle refinements and dramatic changes. A mid-range value offers a good compromise between maintaining the structure of the input and introducing creative variations.</p>",
        "video": "public/settings/denoising.mp4",
        "initialSliderIndex": 10,
        "slidervalue": [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
    }
]